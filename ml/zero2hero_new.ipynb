{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Python310\\\\python.exe'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sys.executable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sqlite3\n",
    "from Bio import SeqIO\n",
    "from Bio.SeqUtils.ProtParam import ProteinAnalysis\n",
    "import json\n",
    "import os\n",
    "import sys\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### basic filtering on both files (not for final product)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_raw_file = \"data/uniprotkb_aeromonas_AND_length_801_TO_2023_11_05.json\" #json input file from uniprot download\n",
    "json_filtered_file = \"data/json_filtered_file.json\" #json output file for filtered proteins\n",
    "fasta_raw_file = \"data/uniprotkb_aeromonas_AND_length_801_TO_2023_11_05.fasta\" #fasta input file from uniprot download\n",
    "fasta_filtered_file = \"data/fasta_filtered_file.fasta\" #fasta output file for filtered proteins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2487\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "# Compiled regular expression for performance\n",
    "x_re = re.compile(r\"X\", re.IGNORECASE)\n",
    "\n",
    "# Loading the data directly without a separate results variable\n",
    "with open(json_raw_file, 'r') as file:\n",
    "    results = json.load(file)[\"results\"]\n",
    "\n",
    "filtered_results = [result for result in results if int(result[\"annotationScore\"]) > 2 and not x_re.search(result[\"sequence\"][\"value\"])]\n",
    "\n",
    "with open(json_filtered_file, 'w') as json_file:\n",
    "    json.dump({\"results\": filtered_results}, json_file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2487\n",
      "2487\n"
     ]
    }
   ],
   "source": [
    "# Reading the FASTA file\n",
    "in_fasta_proteins = list(SeqIO.parse(fasta_raw_file, \"fasta\"))\n",
    "\n",
    "# Extracting protein IDs from json_results and converting to a set for efficient lookups\n",
    "protein_ids = set(result[\"primaryAccession\"] for result in filtered_results)\n",
    "\n",
    "# Filtering in_fasta_proteins based on protein_ids\n",
    "out_fasta_proteins = [protein for protein in in_fasta_proteins if protein.id.split(\"|\")[1].strip() in protein_ids]\n",
    "\n",
    "# Writing the filtered proteins to a new FASTA file\n",
    "with open(fasta_filtered_file, \"w\") as output_handle:\n",
    "    SeqIO.write(out_fasta_proteins, output_handle, \"fasta\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### populate proteins table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "bunu hemen göndermeye gerek yok. gereksiz şekilde kullanmayacağımız fastaları vs. gönderiyoruz. sadece modelleri üretirken. son üründe hepsini göndermek gerekiyor tabii ki."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to the SQLite database (it will be created if it doesn't exist)\n",
    "conn = sqlite3.connect('data/aeromonas1.db')\n",
    "cur = conn.cursor()\n",
    "\n",
    "# drop proteins table\n",
    "cur.execute('''DROP TABLE IF EXISTS proteins''')\n",
    "\n",
    "# Create the table with the necessary columns\n",
    "cur.execute('''CREATE TABLE IF NOT EXISTS proteins (\n",
    "    seq_id TEXT PRIMARY KEY,\n",
    "    description TEXT,\n",
    "    sequence TEXT,\n",
    "    length INTEGER,\n",
    "    molecular_weight REAL,\n",
    "    instability_index REAL,\n",
    "    isoelectric_point REAL,\n",
    "    gravy REAL,\n",
    "    amino_count TEXT,\n",
    "    aromaticity REAL,\n",
    "    flexibility TEXT,\n",
    "    secondary_structure_fraction TEXT,\n",
    "    molar_extinction_coefficient TEXT\n",
    ")''')\n",
    "\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to the SQLite database (it will be created if it doesn't exist)\n",
    "conn = sqlite3.connect('data/aeromonas1.db')\n",
    "cur = conn.cursor()\n",
    "\n",
    "def insert_proteins(records):\n",
    "    # Prepare bulk insert statements\n",
    "    cur.executemany('''INSERT INTO proteins (seq_id, description, sequence, length, molecular_weight, \n",
    "    instability_index, isoelectric_point, gravy, amino_count, aromaticity, flexibility, \n",
    "    secondary_structure_fraction, molar_extinction_coefficient) \n",
    "    VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)''', records)\n",
    "\n",
    "# It's often a good idea to disable journaling and synchronous writes for large bulk inserts.\n",
    "# Don't forget to enable them again if needed for your application's integrity requirements.\n",
    "conn.execute('PRAGMA journal_mode = OFF')\n",
    "conn.execute('PRAGMA synchronous = OFF')\n",
    "\n",
    "# You can also increase cache size\n",
    "conn.execute('PRAGMA cache_size = 3000') # Adjust the cache size to your needs.\n",
    "\n",
    "# Collect records in batches\n",
    "batch_size = 500  # You can tune this number\n",
    "batch_records = []\n",
    "\n",
    "with open(fasta_filtered_file, 'r') as fasta_file:\n",
    "    for record in SeqIO.parse(fasta_file, 'fasta'):\n",
    "        sequence = str(record.seq)\n",
    "        seq_id = record.id.split(\"|\")[1].strip()\n",
    "        length = len(sequence)\n",
    "        description = record.description\n",
    "\n",
    "        placeholders = (None,) * 9\n",
    "\n",
    "        if \"X\" in sequence or \"U\" in sequence:\n",
    "            batch_records.append((seq_id, description, sequence, length) + placeholders)\n",
    "        else:\n",
    "            a_seq = ProteinAnalysis(sequence)\n",
    "            m_weight = a_seq.molecular_weight()\n",
    "            instab_index = a_seq.instability_index()\n",
    "            isoele_point = a_seq.isoelectric_point()\n",
    "            gravy = a_seq.gravy()\n",
    "            amino_count = a_seq.count_amino_acids()\n",
    "            amino_count_json = json.dumps(amino_count)\n",
    "            aromaticity = a_seq.aromaticity()\n",
    "            flexibility = a_seq.flexibility()\n",
    "            flexibility_json = json.dumps(flexibility)\n",
    "            sec_struct_frac = a_seq.secondary_structure_fraction()\n",
    "            sec_struct_frac_json = json.dumps(sec_struct_frac)\n",
    "            ext_coeff = a_seq.molar_extinction_coefficient()\n",
    "            ext_coeff_json = json.dumps(ext_coeff)\n",
    "\n",
    "            batch_records.append((seq_id, description, sequence, length, m_weight, instab_index, isoele_point, gravy, amino_count_json, aromaticity, flexibility_json, sec_struct_frac_json, ext_coeff_json))\n",
    "        \n",
    "        # Insert in batches\n",
    "        if len(batch_records) >= batch_size:\n",
    "            insert_proteins(batch_records)\n",
    "            batch_records = []  # Reset batch\n",
    "\n",
    "# Insert any remaining records\n",
    "if batch_records:\n",
    "    insert_proteins(batch_records)\n",
    "\n",
    "# Commit and clean up\n",
    "conn.commit()\n",
    "conn.execute('PRAGMA journal_mode = DELETE') # Set it back to default or your preferred mode\n",
    "conn.execute('PRAGMA synchronous = NORMAL') # Set it back to default or your preferred mode\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### populate metadata table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = sqlite3.connect('data/aeromonas1.db')\n",
    "cur = conn.cursor()\n",
    "\n",
    "# Drop the 'metadata' table if it exists\n",
    "cur.execute('''DROP TABLE IF EXISTS metadata''')\n",
    "conn.commit()  # Commit the changes\n",
    "\n",
    "# Create the 'metadata' table\n",
    "cur.execute('''\n",
    "    CREATE TABLE metadata (\n",
    "        id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "        primaryAccession TEXT,\n",
    "        annotationScore INTEGER,\n",
    "        comments TEXT,\n",
    "        keywords TEXT,\n",
    "        uniProtKBCrossReferences TEXT\n",
    "    )\n",
    "''')\n",
    "conn.commit()  # Commit the changes\n",
    "\n",
    "#close connection\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import json\n",
    "\n",
    "def insert_records(records):\n",
    "    # Prepare the data for insertion, ensuring that all list or dict types are converted to JSON strings\n",
    "    prepared_records = []\n",
    "    for record in records:\n",
    "        prepared_record = tuple(\n",
    "            json.dumps(item) if isinstance(item, (list, dict)) else item\n",
    "            for item in record\n",
    "        )\n",
    "        prepared_records.append(prepared_record)\n",
    "\n",
    "    # Use executemany to insert all records in one go\n",
    "    cur.executemany('''\n",
    "        INSERT INTO metadata (\n",
    "            primaryAccession, \n",
    "            annotationScore, \n",
    "            comments, \n",
    "            keywords, \n",
    "            uniProtKBCrossReferences\n",
    "        ) VALUES (?, ?, ?, ?, ?)\n",
    "    ''', prepared_records)\n",
    "    conn.commit()\n",
    "\n",
    "# Begin the transaction\n",
    "conn = sqlite3.connect('data/aeromonas1.db')\n",
    "cur = conn.cursor()\n",
    "\n",
    "# Disable journaling and synchronous writes for the duration of the insert operation\n",
    "cur.execute('PRAGMA journal_mode = OFF')\n",
    "cur.execute('PRAGMA synchronous = OFF')\n",
    "cur.execute('PRAGMA cache_size = 1000')  # Increase cache size\n",
    "\n",
    "# Assuming 'your_json_file.json' is in the same directory as your script\n",
    "with open(json_filtered_file, 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "batch_size = 1000  # Define a suitable batch size\n",
    "batch_records = []\n",
    "\n",
    "results = data.get('results', [])\n",
    "\n",
    "for item in results:\n",
    "    record = (\n",
    "        item.get('primaryAccession'),\n",
    "        item.get('annotationScore'),\n",
    "        json.dumps(item.get('comments')) if isinstance(item.get('comments'), list) else item.get('comments'),\n",
    "        json.dumps(item.get('keywords')) if isinstance(item.get('keywords'), list) else item.get('keywords'),\n",
    "        json.dumps(item.get('uniProtKBCrossReferences')) if isinstance(item.get('uniProtKBCrossReferences'), list) else item.get('uniProtKBCrossReferences'),\n",
    "    )\n",
    "    batch_records.append(record)\n",
    "    \n",
    "    if len(batch_records) >= batch_size:\n",
    "        insert_records(batch_records)\n",
    "        batch_records = []  # Reset the batch\n",
    "\n",
    "# Insert any remaining records\n",
    "if batch_records:\n",
    "    insert_records(batch_records)\n",
    "\n",
    "# Commit the transaction\n",
    "conn.commit()\n",
    "\n",
    "# Re-enable journaling and synchronous writes\n",
    "cur.execute('PRAGMA journal_mode = DELETE')  # Set it back to the default or your preferred mode\n",
    "cur.execute('PRAGMA synchronous = NORMAL')   # Set it back to the default or your preferred mode\n",
    "\n",
    "# Close the connection\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### send iFeature tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a SQLite database connection\n",
    "conn = sqlite3.connect('data/aeromonas1.db')\n",
    "\n",
    "# Specify the path to the directory containing your TSV files\n",
    "tsv_directory = 'data/iFeature/'\n",
    "\n",
    "# Iterate through each TSV file in the directory\n",
    "for tsv_filename in os.listdir(tsv_directory):\n",
    "    if tsv_filename.endswith('.tsv'):\n",
    "        # Construct the full file path\n",
    "        file_path = os.path.join(tsv_directory, tsv_filename)\n",
    "        \n",
    "        # Read the TSV file into a DataFrame\n",
    "        df = pd.read_csv(file_path, sep='\\t', encoding='utf-8')\n",
    "\n",
    "        # Extract protein_id and remove the '#' column\n",
    "        df['protein_id'] = df['#'].apply(lambda x: x.split(\"|\")[1].strip())\n",
    "        \n",
    "        # Get the file name without the .tsv extension\n",
    "        file_name = tsv_filename[:-4]\n",
    "        \n",
    "        # Add the file name as a prefix to all columns except 'protein_id'\n",
    "        df.columns = [f\"{file_name}_{col}\" if col != 'protein_id' else col for col in df.columns]\n",
    "        \n",
    "        # Use 'to_sql' method to write the DataFrame to the SQLite table\n",
    "        df.to_sql(file_name, conn, if_exists='replace', index=False)  # Use 'append' if you want to add to an existing table\n",
    "\n",
    "# Close the connection\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### merge all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to the SQLite database (it will be created if it doesn't exist)\n",
    "conn = sqlite3.connect('data/aeromonas1.db')\n",
    "cur = conn.cursor()\n",
    "\n",
    "# Create the table with the necessary columns\n",
    "cur.execute('''\n",
    "CREATE TABLE new_merged_data_filtered AS\n",
    "SELECT\n",
    "    m.*,\n",
    "    p.*,\n",
    "    CTDC.*,\n",
    "    CTDD.*,\n",
    "    CTDT.*,\n",
    "    CTriad.*,\n",
    "    DPC.*,\n",
    "    GAAC.*\n",
    "FROM\n",
    "    metadata m\n",
    "    LEFT JOIN proteins p ON m.primaryAccession = p.seq_id\n",
    "    LEFT JOIN CTDC ON m.primaryAccession = CTDC.protein_id\n",
    "    LEFT JOIN CTDD ON m.primaryAccession = CTDD.protein_id\n",
    "    LEFT JOIN CTDT ON m.primaryAccession = CTDT.protein_id\n",
    "    LEFT JOIN CTriad ON m.primaryAccession = CTriad.protein_id\n",
    "    LEFT JOIN DPC ON m.primaryAccession = DPC.protein_id\n",
    "    LEFT JOIN GAAC ON m.primaryAccession = GAAC.protein_id;\n",
    "''')\n",
    "\n",
    "#drop tables\n",
    "cur.execute('''DROP TABLE IF EXISTS CTDC''')\n",
    "cur.execute('''DROP TABLE IF EXISTS CTDD''')\n",
    "cur.execute('''DROP TABLE IF EXISTS CTDT''')\n",
    "cur.execute('''DROP TABLE IF EXISTS CTriad''')\n",
    "cur.execute('''DROP TABLE IF EXISTS DPC''')\n",
    "cur.execute('''DROP TABLE IF EXISTS GAAC''')\n",
    "cur.execute('''DROP TABLE IF EXISTS proteins''')\n",
    "cur.execute('''DROP TABLE IF EXISTS metadata''')\n",
    "\n",
    "\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### some filtering (son ürün için geçerli)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>COUNT(*)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2224</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   COUNT(*)\n",
       "0      2224"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# run sql query using pandas on a database\n",
    "\n",
    "# Create a SQLite database connection\n",
    "conn = sqlite3.connect('data/aeromonas1.db')\n",
    "\n",
    "# Define the SQL query\n",
    "query = '''\n",
    "---sql\n",
    "SELECT COUNT(*)\n",
    "FROM new_merged_data_filtered\n",
    "WHERE annotationScore IN (3, 4, 5)\n",
    "AND sequence NOT LIKE '%X%'\n",
    "AND uniProtKBCrossReferences LIKE '%GO:%'\n",
    "AND (\n",
    "  uniProtKBCrossReferences LIKE '%\"id\": \"GO:0051287\"%' OR\n",
    "  uniProtKBCrossReferences LIKE '%\"id\": \"GO:0000287\"%' OR\n",
    "  uniProtKBCrossReferences LIKE '%\"id\": \"GO:0003677\"%' OR\n",
    "  uniProtKBCrossReferences LIKE '%\"id\": \"GO:0003723\"%' OR\n",
    "  uniProtKBCrossReferences LIKE '%\"id\": \"GO:0005506\"%' OR\n",
    "  uniProtKBCrossReferences LIKE '%\"id\": \"GO:0005524\"%' OR\n",
    "  uniProtKBCrossReferences LIKE '%\"id\": \"GO:0005525\"%' OR\n",
    "  uniProtKBCrossReferences LIKE '%\"id\": \"GO:0008270\"%' OR\n",
    "  uniProtKBCrossReferences LIKE '%\"id\": \"GO:0016887\"%' OR\n",
    "  uniProtKBCrossReferences LIKE '%\"id\": \"GO:0019843\"%' OR\n",
    "  uniProtKBCrossReferences LIKE '%\"id\": \"GO:0030170\"%' OR\n",
    "  uniProtKBCrossReferences LIKE '%\"id\": \"GO:0046872\"%' OR\n",
    "  uniProtKBCrossReferences LIKE '%\"id\": \"GO:0050661\"%' OR\n",
    "  uniProtKBCrossReferences LIKE '%\"id\": \"GO:0051287\"%' OR\n",
    "  uniProtKBCrossReferences LIKE '%\"id\": \"GO:0051539\"%'\n",
    ")\n",
    "'''\n",
    "\n",
    "# Execute the query\n",
    "df = pd.read_sql_query(query, conn)\n",
    "\n",
    "# Close the database connection\n",
    "conn.close()\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### final table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = sqlite3.connect('data/aeromonas1.db')\n",
    "cur = conn.cursor()\n",
    "\n",
    "cur.executescript('''\n",
    "---sql\n",
    "CREATE TABLE new_merged_data_filtered AS\n",
    "SELECT\n",
    "    *,\n",
    "    (\n",
    "      SELECT json_group_array(json_extract(loc.value, '$.location.value'))\n",
    "      FROM json_each(json_extract(t.comments, '$')) AS entry\n",
    "      LEFT JOIN json_each(json_extract(entry.value, '$.subcellularLocations')) AS loc\n",
    "      WHERE json_extract(entry.value, '$.commentType') = 'SUBCELLULAR LOCATION'\n",
    "    ) AS location_values\n",
    "FROM new_merged_data_filtered AS t;\n",
    "\n",
    "---sql\n",
    "drop table new_merged_data_filtered;\n",
    "\n",
    "ALTER TABLE new_merged_data_filtered ADD COLUMN cytoplasm INTEGER;\n",
    "ALTER TABLE new_merged_data_filtered ADD COLUMN cell_membrane INTEGER;\n",
    "ALTER TABLE new_merged_data_filtered ADD COLUMN cell_wall INTEGER;\n",
    "ALTER TABLE new_merged_data_filtered ADD COLUMN secreted INTEGER;\n",
    "ALTER TABLE new_merged_data_filtered ADD COLUMN periplasm INTEGER;\n",
    "ALTER TABLE new_merged_data_filtered ADD COLUMN cell_surface INTEGER;\n",
    "ALTER TABLE new_merged_data_filtered ADD COLUMN cell_envelope INTEGER;\n",
    "ALTER TABLE new_merged_data_filtered ADD COLUMN chlorosome INTEGER;\n",
    "ALTER TABLE new_merged_data_filtered ADD COLUMN cellular_thylakoid_membrane INTEGER;\n",
    "ALTER TABLE new_merged_data_filtered ADD COLUMN cellular_chromatopore_membrane INTEGER;\n",
    "ALTER TABLE new_merged_data_filtered ADD COLUMN single_pass_membrane_protein INTEGER;\n",
    "ALTER TABLE new_merged_data_filtered ADD COLUMN multi_pass_membrane_protein INTEGER;\n",
    "ALTER TABLE new_merged_data_filtered ADD COLUMN peripheral_membrane_protein INTEGER;\n",
    "\n",
    "---sql\n",
    "UPDATE new_merged_data_filtered\n",
    "SET\n",
    "  cytoplasm = CASE \n",
    "    WHEN location_values LIKE '%\"Cytoplasm\"%' \n",
    "      OR location_values LIKE '%\"Cytoplasmic side\"%' \n",
    "      OR location_values LIKE '%\"Cytoplasm, nucleoid\"%' THEN 1 ELSE 0 END,\n",
    "  cell_membrane = CASE \n",
    "    WHEN location_values LIKE '%\"Cell membrane\"%' \n",
    "      OR location_values LIKE '%\"Membrane\"%' \n",
    "      OR location_values LIKE '%\"Cell inner membrane\"%' \n",
    "      OR location_values LIKE '%\"Cell outer membrane\"%' THEN 1 ELSE 0 END,\n",
    "  cell_wall = CASE WHEN location_values LIKE '%\"Cell wall\"%' THEN 1 ELSE 0 END,\n",
    "  secreted = CASE WHEN location_values LIKE '%\"Secreted\"%' THEN 1 ELSE 0 END,\n",
    "  periplasm = CASE \n",
    "    WHEN location_values LIKE '%\"Periplasm\"%' \n",
    "      OR location_values LIKE '%\"Periplasmic side\"%' THEN 1 ELSE 0 END,\n",
    "  cell_surface = CASE WHEN location_values LIKE '%\"Cell surface\"%' THEN 1 ELSE 0 END,\n",
    "  cell_envelope = CASE WHEN location_values LIKE '%\"Cell envelope\"%' THEN 1 ELSE 0 END,\n",
    "  chlorosome = CASE WHEN location_values LIKE '%\"Chlorosome\"%' THEN 1 ELSE 0 END,\n",
    "  cellular_thylakoid_membrane = CASE WHEN location_values LIKE '%\"Cellular thylakoid membrane\"%' THEN 1 ELSE 0 END,\n",
    "  cellular_chromatopore_membrane = CASE WHEN location_values LIKE '%\"Cellular chromatopore membrane\"%' THEN 1 ELSE 0 END,\n",
    "  single_pass_membrane_protein = CASE WHEN location_values LIKE '%\"Single-pass membrane protein\"%' THEN 1 ELSE 0 END,\n",
    "  multi_pass_membrane_protein = CASE WHEN location_values LIKE '%\"Multi-pass membrane protein\"%' THEN 1 ELSE 0 END,\n",
    "  peripheral_membrane_protein = CASE WHEN location_values LIKE '%\"Peripheral membrane protein\"%' THEN 1 ELSE 0 END;\n",
    "\n",
    "''')\n",
    "\n",
    "conn.commit()\n",
    "\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = sqlite3.connect('data/aeromonas1.db')\n",
    "cur = conn.cursor()\n",
    "\n",
    "cur.executescript('''\n",
    "--- data binding\n",
    "ALTER TABLE new_merged_data_filtered ADD COLUMN dna_binding INTEGER;\n",
    "ALTER TABLE new_merged_data_filtered ADD COLUMN rna_binding INTEGER;\n",
    "ALTER TABLE new_merged_data_filtered ADD COLUMN metal_binding INTEGER;\n",
    "\n",
    "UPDATE c\n",
    "SET\n",
    "  dna_binding = CASE WHEN keywords LIKE '%\"DNA-binding\"%' THEN 1 ELSE 0 END,\n",
    "  rna_binding = CASE WHEN keywords LIKE '%\"RNA-binding\"%' THEN 1 ELSE 0 END,\n",
    "  metal_binding = CASE WHEN keywords LIKE '%\"Metal-binding\"%' THEN 1 ELSE 0 END;\n",
    "\n",
    "''')\n",
    "\n",
    "conn.commit()\n",
    "\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = sqlite3.connect('data/aeromonas1.db')\n",
    "cur = conn.cursor()\n",
    "\n",
    "cur.executescript('''\n",
    "ALTER TABLE new_merged_data_filtered ADD COLUMN GO_0051287 INTEGER DEFAULT 0;\n",
    "ALTER TABLE new_merged_data_filtered ADD COLUMN GO_0009331 INTEGER DEFAULT 0;\n",
    "ALTER TABLE new_merged_data_filtered ADD COLUMN GO_0003677 INTEGER DEFAULT 0;\n",
    "ALTER TABLE new_merged_data_filtered ADD COLUMN GO_0003723 INTEGER DEFAULT 0;\n",
    "ALTER TABLE new_merged_data_filtered ADD COLUMN GO_0005506 INTEGER DEFAULT 0;\n",
    "ALTER TABLE new_merged_data_filtered ADD COLUMN GO_0005524 INTEGER DEFAULT 0;\n",
    "ALTER TABLE new_merged_data_filtered ADD COLUMN GO_0046167 INTEGER DEFAULT 0;\n",
    "ALTER TABLE new_merged_data_filtered ADD COLUMN GO_0008270 INTEGER DEFAULT 0;\n",
    "ALTER TABLE new_merged_data_filtered ADD COLUMN GO_0016887 INTEGER DEFAULT 0;\n",
    "ALTER TABLE new_merged_data_filtered ADD COLUMN GO_0019843 INTEGER DEFAULT 0;\n",
    "ALTER TABLE new_merged_data_filtered ADD COLUMN GO_0008654 INTEGER DEFAULT 0;\n",
    "ALTER TABLE new_merged_data_filtered ADD COLUMN GO_0046872 INTEGER DEFAULT 0;\n",
    "ALTER TABLE new_merged_data_filtered ADD COLUMN GO_0050661 INTEGER DEFAULT 0;\n",
    "ALTER TABLE new_merged_data_filtered ADD COLUMN GO_0051539 INTEGER DEFAULT 0;\n",
    "\n",
    "\n",
    "UPDATE new_merged_data_filtered\n",
    "SET\n",
    "  GO_0051287 = CASE WHEN uniProtKBCrossReferences LIKE '%\"GO:0051287\"%' THEN 1 ELSE 0 END,\n",
    "  GO_0009331 = CASE WHEN uniProtKBCrossReferences LIKE '%\"GO:0009331\"%' THEN 1 ELSE 0 END,\n",
    "  GO_0003677 = CASE WHEN uniProtKBCrossReferences LIKE '%\"GO:0003677\"%' THEN 1 ELSE 0 END,\n",
    "  GO_0003723 = CASE WHEN uniProtKBCrossReferences LIKE '%\"GO:0003723\"%' THEN 1 ELSE 0 END,\n",
    "  GO_0005506 = CASE WHEN uniProtKBCrossReferences LIKE '%\"GO:0005506\"%' THEN 1 ELSE 0 END,\n",
    "  GO_0005524 = CASE WHEN uniProtKBCrossReferences LIKE '%\"GO:0005524\"%' THEN 1 ELSE 0 END,\n",
    "  GO_0046167 = CASE WHEN uniProtKBCrossReferences LIKE '%\"GO:0046167\"%' THEN 1 ELSE 0 END,\n",
    "  GO_0008270 = CASE WHEN uniProtKBCrossReferences LIKE '%\"GO:0008270\"%' THEN 1 ELSE 0 END,\n",
    "  GO_0016887 = CASE WHEN uniProtKBCrossReferences LIKE '%\"GO:0016887\"%' THEN 1 ELSE 0 END,\n",
    "  GO_0019843 = CASE WHEN uniProtKBCrossReferences LIKE '%\"GO:0019843\"%' THEN 1 ELSE 0 END,\n",
    "  GO_0008654 = CASE WHEN uniProtKBCrossReferences LIKE '%\"GO:0008654\"%' THEN 1 ELSE 0 END,\n",
    "  GO_0046872 = CASE WHEN uniProtKBCrossReferences LIKE '%\"GO:0046872\"%' THEN 1 ELSE 0 END,\n",
    "  GO_0050661 = CASE WHEN uniProtKBCrossReferences LIKE '%\"GO:0050661\"%' THEN 1 ELSE 0 END,\n",
    "  GO_0051539 = CASE WHEN uniProtKBCrossReferences LIKE '%\"GO:0051539\"%' THEN 1 ELSE 0 END;\n",
    "''')\n",
    "\n",
    "conn.commit()\n",
    "\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = sqlite3.connect('data/aeromonas1.db')\n",
    "cur = conn.cursor()\n",
    "\n",
    "cur.executescript('''\n",
    "\n",
    "ALTER TABLE new_merged_data_filtered ADD COLUMN A INTEGER;\n",
    "ALTER TABLE new_merged_data_filtered ADD COLUMN C INTEGER;\n",
    "ALTER TABLE new_merged_data_filtered ADD COLUMN D INTEGER;\n",
    "ALTER TABLE new_merged_data_filtered ADD COLUMN E INTEGER;\n",
    "ALTER TABLE new_merged_data_filtered ADD COLUMN F INTEGER;\n",
    "ALTER TABLE new_merged_data_filtered ADD COLUMN G INTEGER;\n",
    "ALTER TABLE new_merged_data_filtered ADD COLUMN H INTEGER;\n",
    "ALTER TABLE new_merged_data_filtered ADD COLUMN I INTEGER;\n",
    "ALTER TABLE new_merged_data_filtered ADD COLUMN K INTEGER;\n",
    "ALTER TABLE new_merged_data_filtered ADD COLUMN L INTEGER;\n",
    "ALTER TABLE new_merged_data_filtered ADD COLUMN M INTEGER;\n",
    "ALTER TABLE new_merged_data_filtered ADD COLUMN N INTEGER;\n",
    "ALTER TABLE new_merged_data_filtered ADD COLUMN P INTEGER;\n",
    "ALTER TABLE new_merged_data_filtered ADD COLUMN Q INTEGER;\n",
    "ALTER TABLE new_merged_data_filtered ADD COLUMN R INTEGER;\n",
    "ALTER TABLE new_merged_data_filtered ADD COLUMN S INTEGER;\n",
    "ALTER TABLE new_merged_data_filtered ADD COLUMN T INTEGER;\n",
    "ALTER TABLE new_merged_data_filtered ADD COLUMN V INTEGER;\n",
    "ALTER TABLE new_merged_data_filtered ADD COLUMN W INTEGER;\n",
    "ALTER TABLE new_merged_data_filtered ADD COLUMN Y INTEGER;\n",
    "\n",
    "UPDATE new_merged_data_filtered\n",
    "SET \n",
    "    A = json_extract(amino_count, '$.A'),\n",
    "    C = json_extract(amino_count, '$.C'),\n",
    "    D = json_extract(amino_count, '$.D'),\n",
    "    E = json_extract(amino_count, '$.E'),\n",
    "    F = json_extract(amino_count, '$.F'),\n",
    "    G = json_extract(amino_count, '$.G'),\n",
    "    H = json_extract(amino_count, '$.H'),\n",
    "    I = json_extract(amino_count, '$.I'),\n",
    "    K = json_extract(amino_count, '$.K'),\n",
    "    L = json_extract(amino_count, '$.L'),\n",
    "    M = json_extract(amino_count, '$.M'),\n",
    "    N = json_extract(amino_count, '$.N'),\n",
    "    P = json_extract(amino_count, '$.P'),\n",
    "    Q = json_extract(amino_count, '$.Q'),\n",
    "    R = json_extract(amino_count, '$.R'),\n",
    "    S = json_extract(amino_count, '$.S'),\n",
    "    T = json_extract(amino_count, '$.T'),\n",
    "    V = json_extract(amino_count, '$.V'),\n",
    "    W = json_extract(amino_count, '$.W'),\n",
    "    Y = json_extract(amino_count, '$.Y');\n",
    "\n",
    "    ''')\n",
    "\n",
    "conn.commit()\n",
    "\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = sqlite3.connect('data/aeromonas1.db')\n",
    "cur = conn.cursor()\n",
    "\n",
    "cur.executescript('''\n",
    "\n",
    "ALTER TABLE new_merged_data_filtered DROP COLUMN primaryAccession;\n",
    "ALTER TABLE new_merged_data_filtered DROP COLUMN annotationScore;\n",
    "ALTER TABLE new_merged_data_filtered DROP COLUMN keywords;\n",
    "ALTER TABLE new_merged_data_filtered DROP COLUMN uniProtKBCrossReferences;\n",
    "ALTER TABLE new_merged_data_filtered DROP COLUMN seq_id;\n",
    "ALTER TABLE new_merged_data_filtered DROP COLUMN description;\n",
    "ALTER TABLE new_merged_data_filtered DROP COLUMN sequence;\n",
    "ALTER TABLE new_merged_data_filtered DROP COLUMN amino_count;\n",
    "ALTER TABLE new_merged_data_filtered DROP COLUMN \"CTDC_#\";\n",
    "ALTER TABLE new_merged_data_filtered DROP COLUMN \"protein_id\";\n",
    "ALTER TABLE new_merged_data_filtered DROP COLUMN \"CTDD_#\";\n",
    "ALTER TABLE new_merged_data_filtered DROP COLUMN \"protein_id:1\";\n",
    "ALTER TABLE new_merged_data_filtered DROP COLUMN \"CTDT_#\";\n",
    "ALTER TABLE new_merged_data_filtered DROP COLUMN \"protein_id:2\";\n",
    "ALTER TABLE new_merged_data_filtered DROP COLUMN \"CTriad_#\";\n",
    "ALTER TABLE new_merged_data_filtered DROP COLUMN \"protein_id:3\";\n",
    "ALTER TABLE new_merged_data_filtered DROP COLUMN \"DPC_#\";\n",
    "ALTER TABLE new_merged_data_filtered DROP COLUMN \"protein_id:4\";\n",
    "ALTER TABLE new_merged_data_filtered DROP COLUMN \"GAAC_#\";\n",
    "ALTER TABLE new_merged_data_filtered DROP COLUMN \"protein_id:2682086651\";\n",
    "ALTER TABLE new_merged_data_filtered DROP COLUMN location_values;\n",
    "ALTER TABLE new_merged_data_filtered DROP COLUMN flexibility;\n",
    "ALTER TABLE new_merged_data_filtered DROP COLUMN secondary_structure_fraction;\n",
    "ALTER TABLE new_merged_data_filtered DROP COLUMN molar_extinction_coefficient;\n",
    "''')\n",
    "\n",
    "conn.commit()\n",
    "\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = sqlite3.connect('data/aeromonas1.db')\n",
    "cur = conn.cursor()\n",
    "\n",
    "cur.executescript('''\n",
    "\n",
    "ALTER TABLE new_merged_data_filtered DROP COLUMN flexibility;\n",
    "ALTER TABLE new_merged_data_filtered DROP COLUMN secondary_structure_fraction;\n",
    "ALTER TABLE new_merged_data_filtered DROP COLUMN molar_extinction_coefficient;\n",
    "\n",
    "''')\n",
    "\n",
    "conn.commit()\n",
    "\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create model and save as pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import warnings\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from skmultilearn.adapt import MLkNN\n",
    "\n",
    "import pickle\n",
    "\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from scipy.sparse import csr_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data from database to pandas dataframe\n",
    "\n",
    "conn = sqlite3.connect('data/aeromonas1.db')\n",
    "cur = conn.cursor()\n",
    "\n",
    "df = pd.read_sql_query(\"SELECT * FROM new_merged_data_filtered\", conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[df.columns[~df.columns.str.startswith('GO_')]]\n",
    "y = df[df.columns[df.columns.str.startswith('GO_')]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_save_model(model, X, y, cv, model_name):\n",
    "    # Suppress FutureWarnings\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "        # Perform cross-validation\n",
    "        accuracy = np.mean(cross_val_score(model, X, y, cv=cv))\n",
    "        print(f\"{model_name} Cross-Validation Accuracy: {accuracy}\")\n",
    "\n",
    "    # Train the model on the entire dataset\n",
    "    model.fit(X, y)\n",
    "\n",
    "    # Save the model to a pickle file\n",
    "    with open(f'models/{model_name}.pkl', 'wb') as file:\n",
    "        pickle.dump(model, file)\n",
    "\n",
    "    print(f\"Model saved as {model_name}.pkl\")\n",
    "\n",
    "# Assuming X and y are your features and labels\n",
    "# Define your models\n",
    "models = {\n",
    "    \"RandomForestClassifier\": RandomForestClassifier(n_estimators=100, random_state=0),\n",
    "    #\"LogisticRegression\": LogisticRegression(random_state=0),\n",
    "    #\"SVC\": SVC(random_state=0),\n",
    "    #\"GradientBoostingClassifier\": GradientBoostingClassifier(n_estimators=100, random_state=0),\n",
    "    \"gb_model\": MultiOutputClassifier(HistGradientBoostingClassifier(random_state=0)),\n",
    "    # \"mlknn_model\": MLkNN(k=10)\n",
    "}\n",
    "\n",
    "if isinstance(y, pd.DataFrame):\n",
    "    y_sparse = csr_matrix(y.values)\n",
    "else:\n",
    "    y_sparse = y  # Assuming y is already in sparse format\n",
    "\n",
    "# Train and save each model\n",
    "for model_name, model in models.items():\n",
    "    train_and_save_model(model, X, y, cv=10, model_name=model_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Load the model from file\n",
    "with open('models/gb_model.pkl', 'rb') as file:\n",
    "    loaded_model = pickle.load(file)\n",
    "\n",
    "# Assuming you have input data prepared as X_test\n",
    "# X_test = ... # Your code to prepare the input data\n",
    "\n",
    "# Make predictions\n",
    "predictions = loaded_model.predict(X)\n",
    "\n",
    "# Output or use the predictions\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_df = pd.DataFrame(predictions, columns=y.columns)\n",
    "predictions_df\n",
    "\n",
    "# give sum of each column\n",
    "predictions_df.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
